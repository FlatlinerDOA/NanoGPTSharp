<?xml version="1.0"?>
<doc>
    <assembly>
        <name>PerceptivePyro</name>
    </assembly>
    <members>
        <member name="P:PerceptivePyro.CausalSelfAttention.bias">
            <summary>
            Gets or sets the bias buffer.
            </summary>
        </member>
        <member name="T:PerceptivePyro.DataSets">
            <summary>
            Provides simplified access to data sets by downloading them first.
            </summary>
        </member>
        <member name="M:PerceptivePyro.DataSets.DownloadDataSetAsync(System.String,System.String,System.Threading.CancellationToken)">
            <summary>
            Attempts to download a data set or just returns it's file path if it has already been downloaded.
            </summary>
            <param name="dataset">The name of the dataset.</param>
            <param name="url">The url to download the data set from if it isn't already.</param>
            <param name="cancellation">Cancels the download.</param>
            <returns>The file path to the downloaded data set.</returns>
        </member>
        <member name="T:PerceptivePyro.DumpExtensions">
            <summary>
            LINQPad style dump extensions for "stringify" nicely and printing out to the console.
            </summary>
        </member>
        <member name="M:PerceptivePyro.DumpExtensions.Stringify(System.Object)">
            <summary>
            Creates a pythonesque print string for any object.
            </summary>
            <param name="item">The object to stringify.</param>
            <returns>A non null string.</returns>
        </member>
        <member name="M:PerceptivePyro.DumpExtensions.Stringify(System.Collections.IEnumerable)">
            <summary>
            Creates a pythonesque print string for a sequence of objects.
            </summary>
            <param name="items">The sequence.</param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.DumpExtensions.Stringify(TorchSharp.Scalar)">
            <summary>
            Creates a pythonesque print string for a Torch scalar.
            </summary>
            <param name="item">The Torch scalar to stringify.</param>
            <returns>A non null string.</returns>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.batch_size">
            <summary>
            how many independent sequences will we process in parallel?
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.block_size">
            <summary>
            What is the maximum context length for predictions?
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.max_iters">
            <summary>
            Upper limit on the number of iterations (steps)
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.eval_interval">
            <summary>
            How many iterations or steps before we evaluate the metrics.
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.learning_rate">
            <summary>
            Learning rate for gradients.
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.eval_iters">
            <summary>
            How many steps should be dedicated to evaluation.
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.n_embd">
            <summary>
            Number of embeddings
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.n_heads">
            <summary>
            Number of attention heads per block
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.n_layers">
            <summary>
            Number of transformer block layers
            </summary>
        </member>
        <member name="F:PerceptivePyro.Examples.BigramLanguageModelExamples.dropout">
            <summary>
            Ratio of dropout for regularization purposes (can help avoid overfitting and smooths out loss curve).
            </summary>
        </member>
        <member name="M:PerceptivePyro.Examples.BigramLanguageModelExamples.Training_On_Shakespeare">
            <summary>
            Demonstrates training a Transformer Language Model on 1MB of William Shakespeare's works.
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Benchmark_MSMARCO">
            <summary>
            Evaluates GPT2 embedding sentence similarity scoring on the MS MARCO V2.1 dataset.
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Benchmark_Sick">
            <summary>
            Evaluates GPT2 embedding sentence similarity scoring on the SICK dataset.
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Gpt2_Embeddings">
            <summary>
            Generates embeddings for a set of sentences.
            124M parameter model (gpt2)
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Gpt2_Large_Embeddings">
            <summary>
            Generates embeddings for a set of sentences.
            774M parameter model (gpt2-large)
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Gpt2_124M_Unconditioned">
            <summary>
            Generates unconditioned (unprompted) random musings by GPT2.
            124M parameter pre-trained model (gpt2)
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Gpt2_Large_Unconditioned">
            <summary>
            Generates unconditioned (unprompted) random musings by GPT2 - 774M parameter pre-trained model (gpt2-large)
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Gpt2_124m_Prompted">
            <summary>
            Generates a prompted response from GPT2 - 124M parameter pre-trained model (gpt2)
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.Gpt2_Large_Prompted">
            <summary>
            Generates a prompted response from GPT2 - 774M parameter pre-trained model (gpt2-large)
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.GPT2_Fine_Tuning">
            <summary>
            Work in Progress - Demonstrates fine tuning GPT2 124M parameter pre-trained model. This one requires a 12GB+ graphics card .
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.GPTExamples.output_embeddings(PerceptivePyro.GPTModel,System.Collections.Generic.IReadOnlyList{System.String},System.String)">
            <summary>
            Gets output embeddings from the layer just before the output layer.
            This gives "Semantic embeddings" of the whole sentence.
            </summary>
            <param name="gpt"></param>
            <param name="prompt"></param>
            <param name="max_length"></param>
            <param name="num_return_sequences"></param>
            <param name="device"></param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.RobertaExamples.Roberta_Tokenizing">
            <summary>
            Tokenizes sentences into numbers with the RobertaTokenizer.
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.RobertaExamples.Roberta_Loading_Pretrained">
            <summary>
            Demonstrates loading of a RobertaModel from pre-trained weights.
            </summary>
        </member>
        <member name="M:PerceptivePyro.Examples.RobertaExamples.Roberta_Sentence_Embeddings">
            <summary>
            Uses all-distilroberta-v1 to create sentence embeddings.
            </summary>
        </member>
        <member name="M:PerceptivePyro.Examples.RobertaExamples.Roberta_Sentence_Similarity">
            <summary>
            Uses all-distilroberta-v1 to create sentence embeddings and calculate their similarity.
            </summary>
        </member>
        <member name="M:PerceptivePyro.Examples.SafeTensorsExamples.Loading_Safe_Tensors">
            <summary>
            Demonstrates loading and inspecting safe tensors.
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.SelfAttentionExamples.self_attention_explained">
            <summary>
            Demonstrates how self attention is constructed, step by step.
            </summary>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.Examples.SemanticDictionaryExamples.Semantic_Dictionary_TopK">
            <summary>
            Indexes a bunch of sentences into a dictionary and then looks up from a similar sentence.
            </summary>
            <returns></returns>
        </member>
        <member name="T:PerceptivePyro.FeedForward">
            <summary>
            Multiple heads of self attention, running in parallel.
            </summary>
        </member>
        <member name="M:PerceptivePyro.GPTModel.get_num_params(System.Boolean)">
            <summary>
            Return the number of parameters in the model.
            For non-embedding count(default), the position embeddings get subtracted.
            The token embeddings would too, except due to the parameter sharing these
            params are actually used as weights in the final layer, so we include them.
            </summary>
            <param name="non_embedding"></param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.GPTModel.crop_block_size(System.Int32)">
            <summary>
            model surgery to decrease the block size if necessary
            e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
            but want to use a smaller block size for some smaller, simpler model
            </summary>
            <param name="new_block_size">New desired block size</param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.GPTModel.generate(TorchSharp.torch.Tensor,System.Int32,System.Double,System.Nullable{System.Int32})">
            <summary>
            Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
            the sequence max_new_tokens times, feeding the predictions back into the model each time.
            Most likely you'll want to make sure to be in model.eval() mode of operation for this.
            </summary>
            <param name="idx"></param>
            <param name="max_new_tokens"></param>
            <param name="temperature"></param>
            <param name="top_k"></param>
            <returns></returns>
        </member>
        <member name="T:PerceptivePyro.Head">
            <summary>
            One head of self attention
            </summary>
        </member>
        <member name="T:PerceptivePyro.ITokenSplitter">
            <summary>
            Splitter should always include the splitting token at the end.
            </summary>
        </member>
        <member name="T:PerceptivePyro.LayerNorm">
            <summary>
            LayerNorm but with an optional bias. PyTorch doesn't support simply hasBias:False
            </summary>
        </member>
        <member name="T:PerceptivePyro.MultiHeadAttention">
            <summary>
            Multiple heads of self attention, running in parallel.
            </summary>
        </member>
        <member name="T:PerceptivePyro.MultiLayerPerceptron">
            <summary>
            MLP
            </summary>
        </member>
        <member name="P:PerceptivePyro.RobertaConfig.hidden_act">
            <summary>
            Gets or inits an activation function or a standard named function e.g "gelu" or "relu" etc.
            </summary>
        </member>
        <member name="M:PerceptivePyro.RobertaEmbeddings.create_position_ids_from_input_ids(TorchSharp.torch.Tensor,TorchSharp.torch.Tensor,System.Int32)">
            <summary>
            Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols
            are ignored. This is modified from fairseq's `utils.make_positions`.
            </summary>
            <param name="input_ids"></param>
            <param name="padding_idx"></param>
            <param name="past_key_values_length"></param>
            <returns></returns>
        </member>
        <member name="T:PerceptivePyro.RobertaModel">
            <summary>
            TorchLib Roberta Transformer (from HuggingFace).
            Code ported from https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py
            </summary>
        </member>
        <member name="P:PerceptivePyro.RobertaModel.MaxInputTokenLength">
            <summary>
            Gets the maximum number of input tokens prior to tokenization, this is the maximum position embedding size, minus the Beginning of Sentence (BOS) and End of Sentence (EOS) tokens.
            </summary>
        </member>
        <member name="M:PerceptivePyro.RobertaModel._convert_head_mask_to_5d(TorchSharp.torch.Tensor,System.Int32,System.Nullable{TorchSharp.torch.ScalarType})">
            <summary>
            -> [num_hidden_layers x batch x num_heads x seq_length x seq_length]
            </summary>
            <param name="head_mask"></param>
            <param name="num_hidden_layers"></param>
            <param name="dtype">Force a data type (optional).</param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.RobertaModel.get_extended_attention_mask(TorchSharp.torch.Tensor,TorchSharp.torch.Tensor,TorchSharp.torch.Device,System.Nullable{TorchSharp.torch.ScalarType})">
            <summary>
            Makes broadcastable attention and causal masks so that future and masked tokens are ignored.
            </summary>
            <param name="attention_mask">Mask with ones indicating tokens to attend to, zeros for tokens to ignore.</param>
            <param name="input_shape">The shape of the input to the model.</param>
            <param name="device"></param>
            <param name="dtype"></param>
            <returns></returns>
            <exception cref="T:System.NotSupportedException"></exception>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="T:PerceptivePyro.SafeTensors">
            <summary>
            .NET 
            </summary>
        </member>
        <member name="M:PerceptivePyro.SafeTensors.LoadFile(System.String,TorchSharp.torch.Device)">
            <summary>
            Loads a .safetensors file into a sequence of named Tensors
            </summary>
            <param name="filename"></param>
            <param name="device"></param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.SafeTensors.DownloadWeightsAsync(System.String)">
            <summary>
            Downloads the .safetensors for a specified model from HuggingFace into the models folder. (Must have a .safetensors model file.)
            </summary>
            <param name="model">The name of the model to download.</param>
            <returns>The full path to the model file.</returns>
            <exception cref="T:System.Net.Http.HttpRequestException">Http request failed.</exception>
            <exception cref="T:System.IO.IOException">Disk access failed to write the file.</exception>
        </member>
        <member name="T:PerceptivePyro.SemanticDictionary`2">
            <summary>
            A semantic dictionary allows searching for nearest neighbours by the semantic content of the <typeparamref name="TValue"/>.
            You provide a content selector function to get the semantic content to be indexed. An embedding is created for that content and then you can perform <see cref="M:PerceptivePyro.SemanticDictionary`2.GetBatchTop(System.Collections.Generic.IReadOnlyList{System.String},System.Int32)"/> to get the top N most similar results. 
            </summary>
            <typeparam name="TKey">The key of the content being indexed.</typeparam>
            <typeparam name="TValue">The content of the content being indexed.</typeparam>
        </member>
        <member name="P:PerceptivePyro.SemanticDictionary`2.QueryChunkSize">
            <summary>
            Gets the size of each query chunk.
            </summary>
        </member>
        <member name="P:PerceptivePyro.SemanticDictionary`2.MaximumCorpusSize">
            <summary>
            Gets the size of each query chunk.
            </summary>
        </member>
        <member name="P:PerceptivePyro.SemanticDictionary`2.CorpusChunkSize">
            <summary>
            Gets the size of each query chunk.
            </summary>
        </member>
        <member name="P:PerceptivePyro.SemanticDictionary`2.ExcerptSize">
            <summary>
            Gets the maximum number of tokens the model can handle as input.
            </summary>
        </member>
        <member name="P:PerceptivePyro.SemanticDictionary`2.IsReIndexRequired">
            <summary>
            Gets a value indicating whether a re-index is required. This occurs when the fill factor of the corpus drops below 75%.
            </summary>
        </member>
        <member name="P:PerceptivePyro.SemanticDictionary`2.MaximumCorpusIndex">
            <summary>
            Gets the maximum number of items that have ever been added to the corpus since it was last cleared or re-indexed.
            </summary>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.Add(`0,`1)">
            <summary>
            Adds or replaces a single item by key (thread-safe).
            </summary>
            <param name="item">The item to be added to the dictionary.</param>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.Add(System.Collections.Generic.KeyValuePair{`0,`1})">
            <summary>
            Adds or replaces a single item by key (thread-safe).
            </summary>
            <param name="item">The item to be added to the dictionary.</param>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.ContainsKey(`0)">
            <summary>
            Gets a value indicating whether the key is present in this dictionary (thread-safe).
            </summary>
            <param name="key">The key to check for.</param>
            <returns>True if found, otherwise false.</returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.Remove(`0)">
            <summary>
            Removes a single item by key (thread-safe).
            </summary>
            <param name="item">The item to be added to the dictionary.</param>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.GetEnumerator">
            <summary>
            Gets an enumerator to the dictionary (WARNING, performs a thread-safe copy of entire list).
            </summary>
            <returns>The enumerator.</returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.System#Collections#IEnumerable#GetEnumerator">
            <summary>
            Gets an enumerator to the dictionary (not thread-safe).
            </summary>
            <returns>The enumerator.</returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.Clear">
            <summary>
            Clears the entire contents of the dictionary (thread-safe).
            </summary>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.Contains(System.Collections.Generic.KeyValuePair{`0,`1})">
            <summary>
            Determines if the key of the item is contained by this dictionary (Important does not check the value) (thread-safe).
            </summary>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.CopyTo(System.Collections.Generic.KeyValuePair{`0,`1}[],System.Int32)">
            <summary>
            Copies the entire contents of the dictionary to part of a target array (thread-safe).
            </summary>
            <param name="array"></param>
            <param name="arrayIndex"></param>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.Remove(System.Collections.Generic.KeyValuePair{`0,`1})">
            <summary>
            Removes the key on the item (thread-safe).
            </summary>
            <param name="item"></param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.Add(System.ValueTuple{`0,`1})">
            <summary>
            Adds or replaces a single item by key (thread-safe).
            </summary>
            <param name="item">The item to be added to the dictionary.</param>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.CreateAsync(System.Func{`1,System.String},System.Int32,System.Int32,System.String,System.String)">
            <summary>
            Loads the tokenizer and the model for creating embeddings.
            </summary>
            <returns>Task for completion.</returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.AddAll(System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{`0,`1}})">
            <summary>
            Adds or replaces a set of items by key.
            </summary>
            <param name="items">The items to be indexed semantically.</param>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.AppendBatchLocked(System.Collections.Generic.IReadOnlyList{System.Collections.Generic.KeyValuePair{`0,`1}},System.Int32)">
            <summary>
            Appends a batch of items at once, assuming thread lock for Write access has already been taken.
            </summary>
            <param name="items">A batch of items.</param>
            <param name="startOffset">The starting offset to add or replace the corpus embeddings from</param>
            <returns>The new starting offset in the corpus.</returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.GetBatchTop(System.Collections.Generic.IReadOnlyList{System.String},System.Int32)">
            <summary>
            Gets the top N search results for each of the provided sentences.
            </summary>
            <param name="sentences"></param>
            <param name="topK"></param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.GetBatchTopLocked(TorchSharp.torch.Tensor,System.Int32)">
            <summary>
            Gets the top N most similar results in batches.
            This method assumes the thread lock is already taken for read access.
            </summary>
            <param name="queryEmbeddings">All query embeddings in the shape of (Batch, Embedding).</param>
            <param name="topK">The top results per batch to return (default is 10).</param>
            <returns>A list of batches of search results.</returns>
        </member>
        <member name="M:PerceptivePyro.SemanticDictionary`2.IndexOfCorpusKeyRange(System.Int32)">
            <summary>
            Determines the start of the range of values for a given offset, or -1 if no range exists.
            </summary>
            <param name="corpusIndex"></param>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.TensorExtensions.NewGelu(TorchSharp.torch.Tensor)">
            <summary>
            Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
            Reference: <a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units(GELU) paper</a>
            </summary>
            <param name="x">Input tensor</param>
            <returns>Tensor of gelu operator applied.</returns>
        </member>
        <member name="M:PerceptivePyro.TensorExtensions.apply_chunking_to_forward(System.Func{TorchSharp.torch.Tensor[],TorchSharp.torch.Tensor},System.Int32,System.Int32,TorchSharp.torch.Tensor[])">
            <summary>
            This function chunks the `input_tensors` into smaller input tensor parts of size `chunk_size` over the dimension
            `chunk_dim`. It then applies a layer `forward_fn` to each chunk independently to save memory.
            If the `forward_fn` is independent across the `chunk_dim` this function will yield the same result as directly
            applying `forward_fn` to `input_tensors`.
            </summary>
            <param name="foward_fn"></param>
            <param name="chunk_size"></param>
            <param name="chunk_dim"></param>
            <param name="input_tensors"></param>
            <typeparam name="TResult"></typeparam>
            <returns></returns>
        </member>
        <member name="M:PerceptivePyro.TensorExtensions.Enumerate2d``1(TorchSharp.torch.Tensor)">
            <summary>
            Enumerates a 2d tensor (matrix), i.e. row by row, into an enumerable sequence of arrays of a specified type.
            </summary>
            <param name="matrix">The 2d tensor.</param>
            <typeparam name="T">The return type expected.</typeparam>
            <returns>An enumerable sequence of arrays</returns>
        </member>
        <member name="T:PerceptivePyro.TensorExtensions.Activation">
            <summary>
            Wraps a function into a Pytorch module so that the model is self describing
            as to what activation function is used, when printing out the model.
            </summary>
        </member>
        <member name="T:PerceptivePyro.TransformerBlock">
            <summary>
            Transformer block: Communication between tokens followed by computation on tokens.
            </summary>
        </member>
    </members>
</doc>
